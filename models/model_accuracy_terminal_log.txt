Evaluating model...
****related*******
              precision    recall  f1-score   support

           0       0.71      0.41      0.52      2174
           1       0.83      0.94      0.89      6935
           2       0.36      0.45      0.40        67

    accuracy                           0.81      9176
   macro avg       0.63      0.60      0.60      9176
weighted avg       0.80      0.81      0.79      9176

accuracy_form_compare_mean() = 0.8125544899738448
****request*******
              precision    recall  f1-score   support

           0       0.90      0.98      0.94      7620
           1       0.84      0.49      0.62      1556

    accuracy                           0.90      9176
   macro avg       0.87      0.74      0.78      9176
weighted avg       0.89      0.90      0.89      9176

accuracy_form_compare_mean() = 0.8979947689625108
****offer*******
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
              precision    recall  f1-score   support

           0       1.00      1.00      1.00      9132
           1       0.00      0.00      0.00        44

    accuracy                           1.00      9176
   macro avg       0.50      0.50      0.50      9176
weighted avg       0.99      1.00      0.99      9176

accuracy_form_compare_mean() = 0.9952048823016565
****aid_related*******
              precision    recall  f1-score   support

           0       0.80      0.83      0.81      5398
           1       0.75      0.69      0.72      3778

    accuracy                           0.78      9176
   macro avg       0.77      0.76      0.77      9176
weighted avg       0.77      0.78      0.78      9176

accuracy_form_compare_mean() = 0.7763731473408892
****medical_help*******
              precision    recall  f1-score   support

           0       0.92      1.00      0.96      8437
           1       0.66      0.07      0.13       739

    accuracy                           0.92      9176
   macro avg       0.79      0.54      0.55      9176
weighted avg       0.90      0.92      0.89      9176

accuracy_form_compare_mean() = 0.9224062772449869
****medical_products*******
              precision    recall  f1-score   support

           0       0.95      1.00      0.98      8709
           1       0.79      0.09      0.16       467

    accuracy                           0.95      9176
   macro avg       0.87      0.54      0.57      9176
weighted avg       0.94      0.95      0.93      9176

accuracy_form_compare_mean() = 0.9523757628596339
****search_and_rescue*******
              precision    recall  f1-score   support

           0       0.98      1.00      0.99      8935
           1       0.80      0.07      0.12       241

    accuracy                           0.98      9176
   macro avg       0.89      0.53      0.55      9176
weighted avg       0.97      0.98      0.96      9176

accuracy_form_compare_mean() = 0.9750435919790759
****security*******
              precision    recall  f1-score   support

           0       0.98      1.00      0.99      9003
           1       0.00      0.00      0.00       173

    accuracy                           0.98      9176
   macro avg       0.49      0.50      0.50      9176
weighted avg       0.96      0.98      0.97      9176

accuracy_form_compare_mean() = 0.9809285091543156
****military*******
              precision    recall  f1-score   support

           0       0.97      1.00      0.98      8895
           1       0.50      0.06      0.10       281

    accuracy                           0.97      9176
   macro avg       0.74      0.53      0.54      9176
weighted avg       0.96      0.97      0.96      9176

accuracy_form_compare_mean() = 0.9693766346992153
****child_alone*******
              precision    recall  f1-score   support

           0       1.00      1.00      1.00      9176

    accuracy                           1.00      9176
   macro avg       1.00      1.00      1.00      9176
weighted avg       1.00      1.00      1.00      9176

accuracy_form_compare_mean() = 1.0
****water*******
              precision    recall  f1-score   support

           0       0.96      1.00      0.98      8599
           1       0.87      0.39      0.54       577

    accuracy                           0.96      9176
   macro avg       0.91      0.69      0.76      9176
weighted avg       0.95      0.96      0.95      9176

accuracy_form_compare_mean() = 0.9580427201394943
****food*******
              precision    recall  f1-score   support

           0       0.95      0.99      0.97      8140
           1       0.87      0.58      0.70      1036

    accuracy                           0.94      9176
   macro avg       0.91      0.78      0.83      9176
weighted avg       0.94      0.94      0.94      9176

accuracy_form_compare_mean() = 0.9426765475152572
****shelter*******
              precision    recall  f1-score   support

           0       0.94      0.99      0.96      8345
           1       0.81      0.36      0.49       831

    accuracy                           0.93      9176
   macro avg       0.88      0.67      0.73      9176
weighted avg       0.93      0.93      0.92      9176

accuracy_form_compare_mean() = 0.9341761115954664
****clothing*******
              precision    recall  f1-score   support

           0       0.99      1.00      0.99      9035
           1       0.68      0.09      0.16       141

    accuracy                           0.99      9176
   macro avg       0.84      0.55      0.58      9176
weighted avg       0.98      0.99      0.98      9176

accuracy_form_compare_mean() = 0.9853966870095903
****money*******
              precision    recall  f1-score   support

           0       0.98      1.00      0.99      8976
           1       0.88      0.04      0.07       200

    accuracy                           0.98      9176
   macro avg       0.93      0.52      0.53      9176
weighted avg       0.98      0.98      0.97      9176

accuracy_form_compare_mean() = 0.9788578901482128
****missing_people*******
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
              precision    recall  f1-score   support

           0       0.99      1.00      0.99      9068
           1       0.00      0.00      0.00       108

    accuracy                           0.99      9176
   macro avg       0.49      0.50      0.50      9176
weighted avg       0.98      0.99      0.98      9176

accuracy_form_compare_mean() = 0.9882301656495205
****refugees*******
              precision    recall  f1-score   support

           0       0.97      1.00      0.98      8859
           1       0.50      0.02      0.04       317

    accuracy                           0.97      9176
   macro avg       0.73      0.51      0.51      9176
weighted avg       0.95      0.97      0.95      9176

accuracy_form_compare_mean() = 0.9654533565823888
****death*******
              precision    recall  f1-score   support

           0       0.96      1.00      0.98      8748
           1       0.88      0.15      0.26       428

    accuracy                           0.96      9176
   macro avg       0.92      0.57      0.62      9176
weighted avg       0.96      0.96      0.95      9176

accuracy_form_compare_mean() = 0.9593504795117699
****other_aid*******
              precision    recall  f1-score   support

           0       0.87      1.00      0.93      7970
           1       0.61      0.03      0.06      1206

    accuracy                           0.87      9176
   macro avg       0.74      0.52      0.50      9176
weighted avg       0.84      0.87      0.82      9176

accuracy_form_compare_mean() = 0.8702048823016565
****infrastructure_related*******
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
              precision    recall  f1-score   support

           0       0.93      1.00      0.97      8573
           1       0.00      0.00      0.00       603

    accuracy                           0.93      9176
   macro avg       0.47      0.50      0.48      9176
weighted avg       0.87      0.93      0.90      9176

accuracy_form_compare_mean() = 0.9342850915431561
****transport*******
              precision    recall  f1-score   support

           0       0.95      1.00      0.98      8744
           1       0.75      0.04      0.08       432

    accuracy                           0.95      9176
   macro avg       0.85      0.52      0.53      9176
weighted avg       0.95      0.95      0.93      9176

accuracy_form_compare_mean() = 0.9542284219703574
****buildings*******
              precision    recall  f1-score   support

           0       0.95      1.00      0.98      8703
           1       0.88      0.11      0.19       473

    accuracy                           0.95      9176
   macro avg       0.92      0.55      0.58      9176
weighted avg       0.95      0.95      0.94      9176

accuracy_form_compare_mean() = 0.9532476024411508
****electricity*******
              precision    recall  f1-score   support

           0       0.98      1.00      0.99      8980
           1       0.67      0.03      0.06       196

    accuracy                           0.98      9176
   macro avg       0.82      0.52      0.52      9176
weighted avg       0.97      0.98      0.97      9176

accuracy_form_compare_mean() = 0.9789668700959023
****tools*******
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
              precision    recall  f1-score   support

           0       0.99      1.00      1.00      9120
           1       0.00      0.00      0.00        56

    accuracy                           0.99      9176
   macro avg       0.50      0.50      0.50      9176
weighted avg       0.99      0.99      0.99      9176

accuracy_form_compare_mean() = 0.993897122929381
****hospitals*******
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
              precision    recall  f1-score   support

           0       0.99      1.00      1.00      9085
           1       0.00      0.00      0.00        91

    accuracy                           0.99      9176
   macro avg       0.50      0.50      0.50      9176
weighted avg       0.98      0.99      0.99      9176

accuracy_form_compare_mean() = 0.9900828247602441
****shops*******
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
              precision    recall  f1-score   support

           0       1.00      1.00      1.00      9139
           1       0.00      0.00      0.00        37

    accuracy                           1.00      9176
   macro avg       0.50      0.50      0.50      9176
weighted avg       0.99      1.00      0.99      9176

accuracy_form_compare_mean() = 0.9959677419354839
****aid_centers*******
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
              precision    recall  f1-score   support

           0       0.99      1.00      0.99      9054
           1       0.00      0.00      0.00       122

    accuracy                           0.99      9176
   macro avg       0.49      0.50      0.50      9176
weighted avg       0.97      0.99      0.98      9176

accuracy_form_compare_mean() = 0.9867044463818657
****other_infrastructure*******
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
c:\Users\VAIO\Desktop\Estudos\Udacity\clone_repo_test\.env\Lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
              precision    recall  f1-score   support

           0       0.96      1.00      0.98      8766
           1       0.00      0.00      0.00       410

    accuracy                           0.96      9176
   macro avg       0.48      0.50      0.49      9176
weighted avg       0.91      0.96      0.93      9176

accuracy_form_compare_mean() = 0.9553182214472538
****weather_related*******
              precision    recall  f1-score   support

           0       0.89      0.95      0.92      6621
           1       0.84      0.69      0.76      2555

    accuracy                           0.88      9176
   macro avg       0.86      0.82      0.84      9176
weighted avg       0.88      0.88      0.87      9176

accuracy_form_compare_mean() = 0.8779424585876199
****floods*******
              precision    recall  f1-score   support

           0       0.95      1.00      0.97      8421
           1       0.90      0.47      0.62       755

    accuracy                           0.95      9176
   macro avg       0.93      0.73      0.80      9176
weighted avg       0.95      0.95      0.95      9176

accuracy_form_compare_mean() = 0.9522667829119442
****storm*******
              precision    recall  f1-score   support

           0       0.95      0.98      0.97      8304
           1       0.75      0.54      0.63       872

    accuracy                           0.94      9176
   macro avg       0.85      0.76      0.80      9176
weighted avg       0.93      0.94      0.93      9176

accuracy_form_compare_mean() = 0.9392981691368788
****fire*******
              precision    recall  f1-score   support

           0       0.99      1.00      0.99      9083
           1       0.50      0.01      0.02        93

    accuracy                           0.99      9176
   macro avg       0.74      0.51      0.51      9176
weighted avg       0.99      0.99      0.99      9176

accuracy_form_compare_mean() = 0.9898648648648649
****earthquake*******
              precision    recall  f1-score   support

           0       0.98      0.99      0.98      8336
           1       0.88      0.77      0.82       840

    accuracy                           0.97      9176
   macro avg       0.93      0.88      0.90      9176
weighted avg       0.97      0.97      0.97      9176

accuracy_form_compare_mean() = 0.9692676547515258
****cold*******
              precision    recall  f1-score   support

           0       0.98      1.00      0.99      8981
           1       0.83      0.05      0.10       195

    accuracy                           0.98      9176
   macro avg       0.91      0.53      0.54      9176
weighted avg       0.98      0.98      0.97      9176

accuracy_form_compare_mean() = 0.9796207497820401
****other_weather*******
              precision    recall  f1-score   support

           0       0.95      1.00      0.97      8698
           1       0.67      0.03      0.06       478

    accuracy                           0.95      9176
   macro avg       0.81      0.51      0.51      9176
weighted avg       0.93      0.95      0.93      9176

accuracy_form_compare_mean() = 0.9486704446381866
****direct_report*******
****direct_report*******
              precision    recall  f1-score   support


           0       0.86      0.98      0.92      7385
           1       0.80      0.34      0.48      1791
           1       0.80      0.34      0.48      1791

    accuracy                           0.85      9176
   macro avg       0.83      0.66      0.70      9176
weighted avg       0.85      0.85      0.83      9176